{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVsoP_xQEzNm"
      },
      "source": [
        "## U-NET\n",
        "\n",
        "Build [U-Net](https://arxiv.org/abs/1505.04597) with [TensorFlow 2](https://www.tensorflow.org/) and train a dataset annotated with [labelme](https://github.com/wkentaro/labelme).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UkZu0gqFJio"
      },
      "source": [
        "### Colab\n",
        "\n",
        "Commands that may need to be executed if you are training in [colab](https://colab.research.google.com/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: apt-get\n",
            "zsh:1: command not found: apt-get\n",
            "Collecting pyside2\n",
            "  Downloading PySide2-5.15.2.1-5.15.2-cp35.cp36.cp37.cp38.cp39.cp310-abi3-macosx_10_13_intel.whl (148.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.5/148.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting shiboken2==5.15.2.1\n",
            "  Downloading shiboken2-5.15.2.1-5.15.2-cp35.cp36.cp37.cp38.cp39.cp310-abi3-macosx_10_13_intel.whl (958 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.6/958.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: shiboken2, pyside2\n",
            "Successfully installed pyside2-5.15.2.1 shiboken2-5.15.2.1\n",
            "Requirement already satisfied: labelme in /Users/jacquelinehui/opt/anaconda3/envs/unet/lib/python3.9/site-packages (5.0.1)\n",
            "Requirement already satisfied: PyYAML in /Users/jacquelinehui/opt/anaconda3/envs/unet/lib/python3.9/site-packages (from labelme) (6.0)\n",
            "Requirement already satisfied: termcolor in /Users/jacquelinehui/opt/anaconda3/envs/unet/lib/python3.9/site-packages (from labelme) (2.2.0)\n",
            "Requirement already satisfied: imgviz>=0.11 in /Users/jacquelinehui/opt/anaconda3/envs/unet/lib/python3.9/site-packages (from labelme) (1.7.2)\n",
            "Requirement already satisfied: PyQt5!=5.15.3,!=5.15.4 in /Users/jacquelinehui/opt/anaconda3/envs/unet/lib/python3.9/site-packages (from labelme) (5.15.9)\n",
            "Requirement already satisfied: numpy in /Users/jacquelinehui/opt/anaconda3/envs/unet/lib/python3.9/site-packages (from labelme) (1.24.2)\n",
            "Requirement already satisfied: Pillow>=2.8 in /Users/jacquelinehui/opt/anaconda3/envs/unet/lib/python3.9/site-packages (from labelme) (9.4.0)\n",
            "Requirement already satisfied: qtpy!=1.11.2 in /Users/jacquelinehui/opt/anaconda3/envs/unet/lib/python3.9/site-packages (from labelme) (2.3.0)\n",
            "Collecting matplotlib<3.3\n",
            "  Using cached matplotlib-3.2.2-cp39-cp39-macosx_10_9_x86_64.whl\n",
            "Requirement already satisfied: natsort>=7.1.0 in /Users/jacquelinehui/opt/anaconda3/envs/unet/lib/python3.9/site-packages (from labelme) (8.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /Users/jacquelinehui/opt/anaconda3/envs/unet/lib/python3.9/site-packages (from matplotlib<3.3->labelme) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/jacquelinehui/opt/anaconda3/envs/unet/lib/python3.9/site-packages (from matplotlib<3.3->labelme) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/jacquelinehui/opt/anaconda3/envs/unet/lib/python3.9/site-packages (from matplotlib<3.3->labelme) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/jacquelinehui/opt/anaconda3/envs/unet/lib/python3.9/site-packages (from matplotlib<3.3->labelme) (3.0.9)\n",
            "Requirement already satisfied: PyQt5-Qt5>=5.15.2 in /Users/jacquelinehui/opt/anaconda3/envs/unet/lib/python3.9/site-packages (from PyQt5!=5.15.3,!=5.15.4->labelme) (5.15.2)\n",
            "Requirement already satisfied: PyQt5-sip<13,>=12.11 in /Users/jacquelinehui/opt/anaconda3/envs/unet/lib/python3.9/site-packages (from PyQt5!=5.15.3,!=5.15.4->labelme) (12.11.1)\n",
            "Requirement already satisfied: packaging in /Users/jacquelinehui/opt/anaconda3/envs/unet/lib/python3.9/site-packages (from qtpy!=1.11.2->labelme) (23.0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/jacquelinehui/opt/anaconda3/envs/unet/lib/python3.9/site-packages (from python-dateutil>=2.1->matplotlib<3.3->labelme) (1.16.0)\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.5.3\n",
            "    Uninstalling matplotlib-3.5.3:\n",
            "      Successfully uninstalled matplotlib-3.5.3\n",
            "Successfully installed matplotlib-3.2.2\n"
          ]
        }
      ],
      "source": [
        "!apt-get install python3-pyqt5\n",
        "!apt-get install python3-pyside\n",
        "!pip install pyside2\n",
        "!pip install labelme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/unet/lib/python3.9/site-packages/tensorflow/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/unet/lib/python3.9/site-packages/tensorflow/python/__init__.py:37\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[39m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tensorflow \u001b[39mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/unet/lib/python3.9/site-packages/tensorflow/python/eager/context.py:29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msix\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m function_pb2\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m config_pb2\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m coordination_config_pb2\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/unet/lib/python3.9/site-packages/tensorflow/core/framework/function_pb2.py:16\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m attr_value_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m node_def_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_node__def__pb2\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m op_def_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_op__def__pb2\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/unet/lib/python3.9/site-packages/tensorflow/core/framework/attr_value_pb2.py:16\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__pb2\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m types_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/unet/lib/python3.9/site-packages/tensorflow/core/framework/tensor_pb2.py:16\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m resource_handle_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m types_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/unet/lib/python3.9/site-packages/tensorflow/core/framework/resource_handle_pb2.py:16\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m types_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n\u001b[1;32m     20\u001b[0m DESCRIPTOR \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mFileDescriptor(\n\u001b[1;32m     21\u001b[0m   name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow/core/framework/resource_handle.proto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     22\u001b[0m   package\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m   ,\n\u001b[1;32m     27\u001b[0m   dependencies\u001b[39m=\u001b[39m[tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[39m.\u001b[39mDESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2\u001b[39m.\u001b[39mDESCRIPTOR,])\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/unet/lib/python3.9/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:36\u001b[0m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[1;32m     18\u001b[0m DESCRIPTOR \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mFileDescriptor(\n\u001b[1;32m     19\u001b[0m   name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow/core/framework/tensor_shape.proto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m   package\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m   serialized_pb\u001b[39m=\u001b[39m_b(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m,tensorflow/core/framework/tensor_shape.proto\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mtensorflow\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mz\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x10\u001b[39;00m\u001b[39mTensorShapeProto\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x03\u001b[39;00m\u001b[39m\\x64\u001b[39;00m\u001b[39mim\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x02\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x03\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m .tensorflow.TensorShapeProto.Dim\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x14\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39munknown_rank\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x03\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x08\u001b[39;00m\u001b[39m\\x1a\u001b[39;00m\u001b[39m!\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x03\u001b[39;00m\u001b[39m\\x44\u001b[39;00m\u001b[39mim\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x04\u001b[39;00m\u001b[39msize\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x03\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x04\u001b[39;00m\u001b[39mname\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x02\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mB\u001b[39m\u001b[39m\\x87\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x18\u001b[39;00m\u001b[39morg.tensorflow.frameworkB\u001b[39m\u001b[39m\\x11\u001b[39;00m\u001b[39mTensorShapeProtosP\u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39mZSgithub.com/tensorflow/tensorflow/tensorflow/go/core/framework/tensor_shape_go_proto\u001b[39m\u001b[39m\\xf8\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m\\x62\u001b[39;00m\u001b[39m\\x06\u001b[39;00m\u001b[39mproto3\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     29\u001b[0m _TENSORSHAPEPROTO_DIM \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mDescriptor(\n\u001b[1;32m     30\u001b[0m   name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDim\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     31\u001b[0m   full_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow.TensorShapeProto.Dim\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m   filename\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     33\u001b[0m   file\u001b[39m=\u001b[39mDESCRIPTOR,\n\u001b[1;32m     34\u001b[0m   containing_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     35\u001b[0m   fields\u001b[39m=\u001b[39m[\n\u001b[0;32m---> 36\u001b[0m     _descriptor\u001b[39m.\u001b[39;49mFieldDescriptor(\n\u001b[1;32m     37\u001b[0m       name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msize\u001b[39;49m\u001b[39m'\u001b[39;49m, full_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtensorflow.TensorShapeProto.Dim.size\u001b[39;49m\u001b[39m'\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     38\u001b[0m       number\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, cpp_type\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, label\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     39\u001b[0m       has_default_value\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, default_value\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     40\u001b[0m       message_type\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, enum_type\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, containing_type\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     41\u001b[0m       is_extension\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, extension_scope\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     42\u001b[0m       serialized_options\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, file\u001b[39m=\u001b[39;49mDESCRIPTOR),\n\u001b[1;32m     43\u001b[0m     _descriptor\u001b[39m.\u001b[39mFieldDescriptor(\n\u001b[1;32m     44\u001b[0m       name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m, full_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow.TensorShapeProto.Dim.name\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     45\u001b[0m       number\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m, cpp_type\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     46\u001b[0m       has_default_value\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, default_value\u001b[39m=\u001b[39m_b(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     47\u001b[0m       message_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, enum_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, containing_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m       is_extension\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, extension_scope\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     49\u001b[0m       serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, file\u001b[39m=\u001b[39mDESCRIPTOR),\n\u001b[1;32m     50\u001b[0m   ],\n\u001b[1;32m     51\u001b[0m   extensions\u001b[39m=\u001b[39m[\n\u001b[1;32m     52\u001b[0m   ],\n\u001b[1;32m     53\u001b[0m   nested_types\u001b[39m=\u001b[39m[],\n\u001b[1;32m     54\u001b[0m   enum_types\u001b[39m=\u001b[39m[\n\u001b[1;32m     55\u001b[0m   ],\n\u001b[1;32m     56\u001b[0m   serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     57\u001b[0m   is_extendable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     58\u001b[0m   syntax\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mproto3\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     59\u001b[0m   extension_ranges\u001b[39m=\u001b[39m[],\n\u001b[1;32m     60\u001b[0m   oneofs\u001b[39m=\u001b[39m[\n\u001b[1;32m     61\u001b[0m   ],\n\u001b[1;32m     62\u001b[0m   serialized_start\u001b[39m=\u001b[39m\u001b[39m149\u001b[39m,\n\u001b[1;32m     63\u001b[0m   serialized_end\u001b[39m=\u001b[39m\u001b[39m182\u001b[39m,\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     66\u001b[0m _TENSORSHAPEPROTO \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mDescriptor(\n\u001b[1;32m     67\u001b[0m   name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTensorShapeProto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     68\u001b[0m   full_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow.TensorShapeProto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m   serialized_end\u001b[39m=\u001b[39m\u001b[39m182\u001b[39m,\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    103\u001b[0m _TENSORSHAPEPROTO_DIM\u001b[39m.\u001b[39mcontaining_type \u001b[39m=\u001b[39m _TENSORSHAPEPROTO\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/unet/lib/python3.9/site-packages/google/protobuf/descriptor.py:561\u001b[0m, in \u001b[0;36mFieldDescriptor.__new__\u001b[0;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m, name, full_name, index, number, \u001b[39mtype\u001b[39m, cpp_type, label,\n\u001b[1;32m    556\u001b[0m             default_value, message_type, enum_type, containing_type,\n\u001b[1;32m    557\u001b[0m             is_extension, extension_scope, options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    558\u001b[0m             serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    559\u001b[0m             has_default_value\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, containing_oneof\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json_name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    560\u001b[0m             file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, create_key\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):  \u001b[39m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m   _message\u001b[39m.\u001b[39;49mMessage\u001b[39m.\u001b[39;49m_CheckCalledFromGeneratedFile()\n\u001b[1;32m    562\u001b[0m   \u001b[39mif\u001b[39;00m is_extension:\n\u001b[1;32m    563\u001b[0m     \u001b[39mreturn\u001b[39;00m _message\u001b[39m.\u001b[39mdefault_pool\u001b[39m.\u001b[39mFindExtensionByName(full_name)\n",
            "\u001b[0;31mTypeError\u001b[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYECmeT2EzNs"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 4 # class number + 1 (background)\n",
        "INPUT_SHAPE = [480, 640, 3] # (H, W, C)\n",
        "BATCH_SIZE = 2\n",
        "EPOCHS = 100\n",
        "VAL_SUBSPLITS = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9o9lfRVEzNs"
      },
      "source": [
        "### Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNwIk5iuEzNt"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "\n",
        "def cvtColor(image):\n",
        "    if len(np.shape(image)) == 3 and np.shape(image)[-2] == 3:\n",
        "        return image\n",
        "    else:\n",
        "        image = image.convert('RGB')\n",
        "        return image\n",
        "\n",
        "\n",
        "def normalize(image):\n",
        "    image = image / 127.5 - 1\n",
        "    return image\n",
        "\n",
        "\n",
        "def resize_image(image, size):\n",
        "    iw, ih = image.size\n",
        "    w, h = size\n",
        "\n",
        "    scale = min(w / iw, h / ih)\n",
        "    nw = int(iw * scale)\n",
        "    nh = int(ih * scale)\n",
        "\n",
        "    image = image.resize((nw, nh), Image.BICUBIC)\n",
        "    new_image = Image.new('RGB', size, (128, 128, 128))\n",
        "    new_image.paste(image, ((w - nw) // 2, (h - nh) // 2))\n",
        "\n",
        "    return new_image, nw, nh\n",
        "\n",
        "\n",
        "def resize_label(image, size):\n",
        "    iw, ih = image.size\n",
        "    w, h = size\n",
        "\n",
        "    scale = min(w / iw, h / ih)\n",
        "    nw = int(iw * scale)\n",
        "    nh = int(ih * scale)\n",
        "\n",
        "    image = image.resize((nw, nh), Image.NEAREST)\n",
        "    new_image = Image.new('L', size, (0))\n",
        "    new_image.paste(image, ((w - nw) // 2, (h - nh) // 2))\n",
        "\n",
        "    return new_image, nw, nh\n",
        "\n",
        "\n",
        "class UnetDataset(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, annotation_lines, input_shape, batch_size, num_classes,\n",
        "                 train, dataset_path):\n",
        "        self.annotation_lines = annotation_lines\n",
        "        self.length = len(self.annotation_lines)\n",
        "        self.input_shape = input_shape\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = num_classes\n",
        "        self.train = train\n",
        "        self.dataset_path = dataset_path\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.annotation_lines) / float(self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        images = []\n",
        "        targets = []\n",
        "        for i in range(index * self.batch_size, (index + 1) * self.batch_size):\n",
        "            i = i % self.length\n",
        "            name = self.annotation_lines[i].split()[0]\n",
        "            jpg = Image.open(\n",
        "                os.path.join(os.path.join(self.dataset_path, \"JPEGImages\"),\n",
        "                             name + \".jpg\"))\n",
        "            png = Image.open(\n",
        "                os.path.join(\n",
        "                    os.path.join(self.dataset_path, \"SegmentationClassPNG\"),\n",
        "                    name + \".png\"))\n",
        "\n",
        "            jpg, png = self.process_data(jpg,\n",
        "                                         png,\n",
        "                                         self.input_shape,\n",
        "                                         random=self.train)\n",
        "\n",
        "            images.append(jpg)\n",
        "            targets.append(png)\n",
        "\n",
        "        images = np.array(images)\n",
        "        targets = np.array(targets)\n",
        "        return images, targets\n",
        "\n",
        "    def rand(self, a=0, b=1):\n",
        "        return np.random.rand() * (b - a) + a\n",
        "\n",
        "    def process_data(self, image, label, input_shape, random=True):\n",
        "        image = cvtColor(image)\n",
        "        label = Image.fromarray(np.array(label))\n",
        "        h, w, _ = input_shape\n",
        "\n",
        "        # resize\n",
        "        image, _, _ = resize_image(image, (w, h))\n",
        "        label, _, _ = resize_label(label, (w, h))\n",
        "\n",
        "        if random:\n",
        "            # flip\n",
        "            flip = self.rand() < .5\n",
        "            if flip:\n",
        "                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "                label = label.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "        # np\n",
        "        image = np.array(image, np.float32)\n",
        "        image = normalize(image)\n",
        "\n",
        "        label = np.array(label)\n",
        "        label[label >= self.num_classes] = self.num_classes\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVZhBNpNFIZE",
        "outputId": "29d77682-e35c-45b4-a7b8-2bc15d1912cb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_hhyDZ_FSNn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/cobb-angle-unet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yTDWYdLv-WB"
      },
      "outputs": [],
      "source": [
        "!apt-get install python3-pyqt5\n",
        "!apt-get install python3-pyside\n",
        "!pip install pyside2\n",
        "!pip install labelme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TGHuntAxKiM",
        "outputId": "7dbbfe88-0482-4eac-e652-d87739ff7353"
      },
      "outputs": [],
      "source": [
        "!chmod +x labelme2voc.py\n",
        "!make re-voc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4HTyQPEFXl8"
      },
      "source": [
        "### Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7E68WMEdEzNq"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/unet/lib/python3.9/site-packages/tensorflow/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/unet/lib/python3.9/site-packages/tensorflow/python/__init__.py:37\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[39m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tensorflow \u001b[39mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/unet/lib/python3.9/site-packages/tensorflow/python/eager/context.py:29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msix\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m function_pb2\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m config_pb2\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m coordination_config_pb2\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/unet/lib/python3.9/site-packages/tensorflow/core/framework/function_pb2.py:16\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m attr_value_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m node_def_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_node__def__pb2\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m op_def_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_op__def__pb2\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/unet/lib/python3.9/site-packages/tensorflow/core/framework/attr_value_pb2.py:16\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__pb2\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m types_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/unet/lib/python3.9/site-packages/tensorflow/core/framework/tensor_pb2.py:16\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m resource_handle_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m types_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/unet/lib/python3.9/site-packages/tensorflow/core/framework/resource_handle_pb2.py:16\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m types_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n\u001b[1;32m     20\u001b[0m DESCRIPTOR \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mFileDescriptor(\n\u001b[1;32m     21\u001b[0m   name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow/core/framework/resource_handle.proto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     22\u001b[0m   package\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m   ,\n\u001b[1;32m     27\u001b[0m   dependencies\u001b[39m=\u001b[39m[tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[39m.\u001b[39mDESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2\u001b[39m.\u001b[39mDESCRIPTOR,])\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/unet/lib/python3.9/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:36\u001b[0m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[1;32m     18\u001b[0m DESCRIPTOR \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mFileDescriptor(\n\u001b[1;32m     19\u001b[0m   name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow/core/framework/tensor_shape.proto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m   package\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m   serialized_pb\u001b[39m=\u001b[39m_b(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m,tensorflow/core/framework/tensor_shape.proto\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mtensorflow\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mz\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x10\u001b[39;00m\u001b[39mTensorShapeProto\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x03\u001b[39;00m\u001b[39m\\x64\u001b[39;00m\u001b[39mim\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x02\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x03\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m .tensorflow.TensorShapeProto.Dim\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x14\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39munknown_rank\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x03\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x08\u001b[39;00m\u001b[39m\\x1a\u001b[39;00m\u001b[39m!\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x03\u001b[39;00m\u001b[39m\\x44\u001b[39;00m\u001b[39mim\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x04\u001b[39;00m\u001b[39msize\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x03\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x04\u001b[39;00m\u001b[39mname\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x02\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mB\u001b[39m\u001b[39m\\x87\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x18\u001b[39;00m\u001b[39morg.tensorflow.frameworkB\u001b[39m\u001b[39m\\x11\u001b[39;00m\u001b[39mTensorShapeProtosP\u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39mZSgithub.com/tensorflow/tensorflow/tensorflow/go/core/framework/tensor_shape_go_proto\u001b[39m\u001b[39m\\xf8\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m\\x62\u001b[39;00m\u001b[39m\\x06\u001b[39;00m\u001b[39mproto3\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     29\u001b[0m _TENSORSHAPEPROTO_DIM \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mDescriptor(\n\u001b[1;32m     30\u001b[0m   name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDim\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     31\u001b[0m   full_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow.TensorShapeProto.Dim\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m   filename\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     33\u001b[0m   file\u001b[39m=\u001b[39mDESCRIPTOR,\n\u001b[1;32m     34\u001b[0m   containing_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     35\u001b[0m   fields\u001b[39m=\u001b[39m[\n\u001b[0;32m---> 36\u001b[0m     _descriptor\u001b[39m.\u001b[39;49mFieldDescriptor(\n\u001b[1;32m     37\u001b[0m       name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msize\u001b[39;49m\u001b[39m'\u001b[39;49m, full_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtensorflow.TensorShapeProto.Dim.size\u001b[39;49m\u001b[39m'\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     38\u001b[0m       number\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, cpp_type\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, label\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     39\u001b[0m       has_default_value\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, default_value\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     40\u001b[0m       message_type\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, enum_type\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, containing_type\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     41\u001b[0m       is_extension\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, extension_scope\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     42\u001b[0m       serialized_options\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, file\u001b[39m=\u001b[39;49mDESCRIPTOR),\n\u001b[1;32m     43\u001b[0m     _descriptor\u001b[39m.\u001b[39mFieldDescriptor(\n\u001b[1;32m     44\u001b[0m       name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m, full_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow.TensorShapeProto.Dim.name\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     45\u001b[0m       number\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m, cpp_type\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     46\u001b[0m       has_default_value\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, default_value\u001b[39m=\u001b[39m_b(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     47\u001b[0m       message_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, enum_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, containing_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m       is_extension\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, extension_scope\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     49\u001b[0m       serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, file\u001b[39m=\u001b[39mDESCRIPTOR),\n\u001b[1;32m     50\u001b[0m   ],\n\u001b[1;32m     51\u001b[0m   extensions\u001b[39m=\u001b[39m[\n\u001b[1;32m     52\u001b[0m   ],\n\u001b[1;32m     53\u001b[0m   nested_types\u001b[39m=\u001b[39m[],\n\u001b[1;32m     54\u001b[0m   enum_types\u001b[39m=\u001b[39m[\n\u001b[1;32m     55\u001b[0m   ],\n\u001b[1;32m     56\u001b[0m   serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     57\u001b[0m   is_extendable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     58\u001b[0m   syntax\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mproto3\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     59\u001b[0m   extension_ranges\u001b[39m=\u001b[39m[],\n\u001b[1;32m     60\u001b[0m   oneofs\u001b[39m=\u001b[39m[\n\u001b[1;32m     61\u001b[0m   ],\n\u001b[1;32m     62\u001b[0m   serialized_start\u001b[39m=\u001b[39m\u001b[39m149\u001b[39m,\n\u001b[1;32m     63\u001b[0m   serialized_end\u001b[39m=\u001b[39m\u001b[39m182\u001b[39m,\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     66\u001b[0m _TENSORSHAPEPROTO \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mDescriptor(\n\u001b[1;32m     67\u001b[0m   name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTensorShapeProto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     68\u001b[0m   full_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow.TensorShapeProto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m   serialized_end\u001b[39m=\u001b[39m\u001b[39m182\u001b[39m,\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    103\u001b[0m _TENSORSHAPEPROTO_DIM\u001b[39m.\u001b[39mcontaining_type \u001b[39m=\u001b[39m _TENSORSHAPEPROTO\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/unet/lib/python3.9/site-packages/google/protobuf/descriptor.py:561\u001b[0m, in \u001b[0;36mFieldDescriptor.__new__\u001b[0;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m, name, full_name, index, number, \u001b[39mtype\u001b[39m, cpp_type, label,\n\u001b[1;32m    556\u001b[0m             default_value, message_type, enum_type, containing_type,\n\u001b[1;32m    557\u001b[0m             is_extension, extension_scope, options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    558\u001b[0m             serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    559\u001b[0m             has_default_value\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, containing_oneof\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json_name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    560\u001b[0m             file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, create_key\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):  \u001b[39m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m   _message\u001b[39m.\u001b[39;49mMessage\u001b[39m.\u001b[39;49m_CheckCalledFromGeneratedFile()\n\u001b[1;32m    562\u001b[0m   \u001b[39mif\u001b[39;00m is_extension:\n\u001b[1;32m    563\u001b[0m     \u001b[39mreturn\u001b[39;00m _message\u001b[39m.\u001b[39mdefault_pool\u001b[39m.\u001b[39mFindExtensionByName(full_name)\n",
            "\u001b[0;31mTypeError\u001b[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsUjgWRiEzNv"
      },
      "outputs": [],
      "source": [
        "dataset_path = 'datasets/train_voc'\n",
        "\n",
        "# read dataset txt files\n",
        "with open(os.path.join(dataset_path, \"ImageSets/Segmentation/train.txt\"),\n",
        "          \"r\",\n",
        "          encoding=\"utf8\") as f:\n",
        "    train_lines = f.readlines()\n",
        "\n",
        "with open(os.path.join(dataset_path, \"ImageSets/Segmentation/val.txt\"),\n",
        "          \"r\",\n",
        "          encoding=\"utf8\") as f:\n",
        "    val_lines = f.readlines()\n",
        "\n",
        "train_batches = UnetDataset(train_lines, INPUT_SHAPE, BATCH_SIZE, NUM_CLASSES,\n",
        "                            True, dataset_path)\n",
        "val_batches = UnetDataset(val_lines, INPUT_SHAPE, BATCH_SIZE, NUM_CLASSES,\n",
        "                          False, dataset_path)\n",
        "\n",
        "STEPS_PER_EPOCH = len(train_lines) // BATCH_SIZE\n",
        "VALIDATION_STEPS = len(val_lines) // BATCH_SIZE // VAL_SUBSPLITS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GK7XASXEzNx"
      },
      "source": [
        "### Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20Wp1W26EzNx"
      },
      "source": [
        "#### Helper methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkgisrEjEzNy"
      },
      "outputs": [],
      "source": [
        "# https://www.tensorflow.org/tutorials/generative/pix2pix\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(\n",
        "        tf.keras.layers.Conv2DTranspose(filters,\n",
        "                                        size,\n",
        "                                        strides=2,\n",
        "                                        padding='same',\n",
        "                                        kernel_initializer=initializer,\n",
        "                                        use_bias=False))\n",
        "\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    if apply_dropout:\n",
        "        result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "    result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pngEQSHEzNy"
      },
      "source": [
        "#### Base model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRDN4hJlEzNz",
        "outputId": "9d8c6472-363a-4cdf-a919-72e624d7c805"
      },
      "outputs": [],
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(input_shape=INPUT_SHAPE,\n",
        "                                               include_top=False)\n",
        "\n",
        "# Use the activations of these layers\n",
        "layer_names = [\n",
        "    'block_1_expand_relu',  # 64x64\n",
        "    'block_3_expand_relu',  # 32x32\n",
        "    'block_6_expand_relu',  # 16x16\n",
        "    'block_13_expand_relu',  # 8x8\n",
        "    'block_16_project',  # 4x4\n",
        "]\n",
        "base_model_outputs = [\n",
        "    base_model.get_layer(name).output for name in layer_names\n",
        "]\n",
        "\n",
        "# Create the feature extraction model\n",
        "down_stack = tf.keras.Model(inputs=base_model.input,\n",
        "                            outputs=base_model_outputs)\n",
        "\n",
        "down_stack.trainable = False\n",
        "\n",
        "up_stack = [\n",
        "    upsample(512, 3),  # 4x4 -> 8x8\n",
        "    upsample(256, 3),  # 8x8 -> 16x16\n",
        "    upsample(128, 3),  # 16x16 -> 32x32\n",
        "    upsample(64, 3),  # 32x32 -> 64x64\n",
        "]\n",
        "\n",
        "\n",
        "def unet_model(output_channels: int):\n",
        "    inputs = tf.keras.layers.Input(shape=INPUT_SHAPE)\n",
        "\n",
        "    # Downsampling through the model\n",
        "    skips = down_stack(inputs)\n",
        "    x = skips[-1]\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    # Upsampling and establishing the skip connections\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        concat = tf.keras.layers.Concatenate()\n",
        "        x = concat([x, skip])\n",
        "\n",
        "    # This is the last layer of the model\n",
        "    last = tf.keras.layers.Conv2DTranspose(filters=output_channels,\n",
        "                                           kernel_size=3,\n",
        "                                           strides=2,\n",
        "                                           padding='same')  #64x64 -> 128x128\n",
        "\n",
        "    x = last(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNA_Led8EzNz"
      },
      "source": [
        "#### Compile\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK4PgMN_EzN0",
        "outputId": "0d203642-650f-4477-d126-833848408f2d"
      },
      "outputs": [],
      "source": [
        "model = unet_model(output_channels=NUM_CLASSES)\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-D7AOgkEzN0"
      },
      "outputs": [],
      "source": [
        "def create_mask(pred_mask):\n",
        "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
        "    pred_mask = pred_mask[..., tf.newaxis]\n",
        "    return pred_mask[0]\n",
        "\n",
        "\n",
        "def show_predictions(dataset=None, num=1):\n",
        "    if dataset:\n",
        "        for image, mask in dataset.take(num):\n",
        "            pred_mask = model.predict(image)\n",
        "            display([image[0], mask[0], create_mask(pred_mask)])\n",
        "    else:\n",
        "        display([\n",
        "            sample_image, sample_mask,\n",
        "            create_mask(model.predict(sample_image[tf.newaxis, ...]))\n",
        "        ])\n",
        "\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "class DisplayCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        clear_output(wait=True)\n",
        "        show_predictions()\n",
        "        print('\\nSample Prediction after epoch {}\\n'.format(epoch + 1))\n",
        "\n",
        "\n",
        "class ModelCheckpointCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self,\n",
        "                 filepath,\n",
        "                 monitor='val_loss',\n",
        "                 verbose=0,\n",
        "                 save_best_only=False,\n",
        "                 save_weights_only=False,\n",
        "                 mode='auto',\n",
        "                 period=1):\n",
        "        super(ModelCheckpointCallback, self).__init__()\n",
        "        self.monitor = monitor\n",
        "        self.verbose = verbose\n",
        "        self.filepath = filepath\n",
        "        self.save_best_only = save_best_only\n",
        "        self.save_weights_only = save_weights_only\n",
        "        self.period = period\n",
        "        self.epochs_since_last_save = 0\n",
        "\n",
        "        if mode not in ['auto', 'min', 'max']:\n",
        "            warnings.warn(\n",
        "                'ModelCheckpoint mode %s is unknown, '\n",
        "                'fallback to auto mode.' % (mode), RuntimeWarning)\n",
        "            mode = 'auto'\n",
        "\n",
        "        if mode == 'min':\n",
        "            self.monitor_op = np.less\n",
        "            self.best = np.Inf\n",
        "        elif mode == 'max':\n",
        "            self.monitor_op = np.greater\n",
        "            self.best = -np.Inf\n",
        "        else:\n",
        "            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n",
        "                self.monitor_op = np.greater\n",
        "                self.best = -np.Inf\n",
        "            else:\n",
        "                self.monitor_op = np.less\n",
        "                self.best = np.Inf\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        self.epochs_since_last_save += 1\n",
        "        if self.epochs_since_last_save >= self.period:\n",
        "            self.epochs_since_last_save = 0\n",
        "            filepath = self.filepath.format(epoch=epoch + 1, **logs)\n",
        "            if self.save_best_only:\n",
        "                current = logs.get(self.monitor)\n",
        "                if current is None:\n",
        "                    warnings.warn(\n",
        "                        'Can save best model only with %s available, '\n",
        "                        'skipping.' % (self.monitor), RuntimeWarning)\n",
        "                else:\n",
        "                    if self.monitor_op(current, self.best):\n",
        "                        if self.verbose > 0:\n",
        "                            print(\n",
        "                                '\\nEpoch %05d: %s improved from %0.5f to %0.5f,'\n",
        "                                ' saving model to %s' %\n",
        "                                (epoch + 1, self.monitor, self.best, current,\n",
        "                                 filepath))\n",
        "                        self.best = current\n",
        "                        if self.save_weights_only:\n",
        "                            self.model.save_weights(filepath, overwrite=True)\n",
        "                        else:\n",
        "                            self.model.save(filepath, overwrite=True)\n",
        "                    else:\n",
        "                        if self.verbose > 0:\n",
        "                            print('\\nEpoch %05d: %s did not improve' %\n",
        "                                  (epoch + 1, self.monitor))\n",
        "            else:\n",
        "                if self.verbose > 0:\n",
        "                    print('\\nEpoch %05d: saving model to %s' %\n",
        "                          (epoch + 1, filepath))\n",
        "                if self.save_weights_only:\n",
        "                    self.model.save_weights(filepath, overwrite=True)\n",
        "                else:\n",
        "                    self.model.save(filepath, overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "1l38NMIyEzN1",
        "outputId": "7eb30652-ac6f-4948-fe8b-45b1e356751a"
      },
      "outputs": [],
      "source": [
        "displayCallback = DisplayCallback()\n",
        "\n",
        "if not os.path.exists('logs'):\n",
        "    os.makedirs('logs')\n",
        "checkpointCallback = ModelCheckpointCallback(\n",
        "    'logs/ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
        "    monitor='val_loss',\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    period=1)\n",
        "\n",
        "model_history = model.fit(train_batches,\n",
        "                          epochs=EPOCHS,\n",
        "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                          validation_steps=VALIDATION_STEPS,\n",
        "                          validation_data=val_batches,\n",
        "                          callbacks=[displayCallback, checkpointCallback])\n",
        "\n",
        "model.save('logs/the-last-model.h5', overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "T2AA2gEsEzN1",
        "outputId": "d11d52e1-e206-48e0-e7d2-3fb5e9692ea7"
      },
      "outputs": [],
      "source": [
        "loss = model_history.history['loss']\n",
        "val_loss = model_history.history['val_loss']\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(model_history.epoch, loss, 'r', label='Training loss')\n",
        "plt.plot(model_history.epoch, val_loss, 'bo', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I29UY45PEzN1"
      },
      "source": [
        "### Load Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uHP0rM1EzN2",
        "outputId": "81f36f49-ac8d-4084-a70b-9c07a34f4272"
      },
      "outputs": [],
      "source": [
        "model_path = 'logs/the-last-model.h5'\n",
        "model.load_weights(model_path)\n",
        "print('{} model loaded.'.format(model_path))\n",
        "\n",
        "import copy\n",
        "\n",
        "colors = [(0, 0, 0), (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128),\n",
        "          (128, 0, 128), (0, 128, 128), (128, 128, 128),\n",
        "          (64, 0, 0), (192, 0, 0), (64, 128, 0), (192, 128, 0), (64, 0, 128),\n",
        "          (192, 0, 128), (64, 128, 128), (192, 128, 128), (0, 64, 0),\n",
        "          (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128),\n",
        "          (128, 64, 12)]\n",
        "\n",
        "\n",
        "def detect_image(image_path):\n",
        "    image = Image.open(image_path)\n",
        "    image = cvtColor(image)\n",
        "\n",
        "    old_img = copy.deepcopy(image)\n",
        "    ori_h = np.array(image).shape[0]\n",
        "    ori_w = np.array(image).shape[1]\n",
        "\n",
        "    image_data, nw, nh = resize_image(image, (INPUT_SHAPE[1], INPUT_SHAPE[0]))\n",
        "\n",
        "    image_data = normalize(np.array(image_data, np.float32))\n",
        "\n",
        "    image_data = np.expand_dims(image_data, 0)\n",
        "\n",
        "    pr = model.predict(image_data)[0]\n",
        "\n",
        "    pr = pr[int((INPUT_SHAPE[0] - nh) // 2) : int((INPUT_SHAPE[0] - nh) // 2 + nh), \\\n",
        "            int((INPUT_SHAPE[1] - nw) // 2) : int((INPUT_SHAPE[1] - nw) // 2 + nw)]\n",
        "\n",
        "    pr = cv2.resize(pr, (ori_w, ori_h), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    pr = pr.argmax(axis=-1)\n",
        "\n",
        "    # seg_img = np.zeros((np.shape(pr)[0], np.shape(pr)[1], 3))\n",
        "    # for c in range(NUM_CLASSES):\n",
        "    #     seg_img[:, :, 0] += ((pr[:, :] == c ) * colors[c][0]).astype('uint8')\n",
        "    #     seg_img[:, :, 1] += ((pr[:, :] == c ) * colors[c][1]).astype('uint8')\n",
        "    #     seg_img[:, :, 2] += ((pr[:, :] == c ) * colors[c][2]).astype('uint8')\n",
        "    seg_img = np.reshape(\n",
        "        np.array(colors, np.uint8)[np.reshape(pr, [-1])], [ori_h, ori_w, -1])\n",
        "\n",
        "    image = Image.fromarray(seg_img)\n",
        "    image = Image.blend(old_img, image, 0.7)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VWmlHE9EzN2"
      },
      "source": [
        "### Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        },
        "id": "3Md_7M3CEzN2",
        "outputId": "adca942a-72d6-457d-e159-013bec1a4fb7"
      },
      "outputs": [],
      "source": [
        "test_image_path = 'datasets/test/1.jpg'\n",
        "\n",
        "image = detect_image(test_image_path)\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "unet.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "5dd595d4dbb398d5a359ba4af7eca4cfd4a6956f67286a410a68e17ab3aa7f42"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 ('od')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
